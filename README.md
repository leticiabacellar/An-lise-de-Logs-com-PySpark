# âš¡ AnÃ¡lise de Logs com PySpark

Este projeto utiliza o poder do **Apache Spark com PySpark** para processar e analisar grandes volumes de dados de log, demonstrando tÃ©cnicas fundamentais de **ETL distribuÃ­do** e anÃ¡lise de comportamento de usuÃ¡rios com dados nÃ£o estruturados.

---

## ğŸ¯ Objetivo

Realizar uma anÃ¡lise eficiente de arquivos de log utilizando **PySpark**, com foco em:

- Processamento em larga escala
- Limpeza e transformaÃ§Ã£o dos dados
- ExtraÃ§Ã£o de insights relevantes
- ExportaÃ§Ã£o dos resultados para CSV

---

## ğŸ› ï¸ Tecnologias e Bibliotecas

- Python 3.x
- Apache Spark (via PySpark)
- Google Colab / Jupyter Notebook
- Pandas (para validaÃ§Ãµes finais)
- CSV como formato de entrada e saÃ­da

---

## ğŸ—‚ï¸ Estrutura do Projeto

- `AnÃ¡lise de Logs com PySpark.ipynb` â€“ Notebook principal com o pipeline completo.
- `/data/logs.csv` â€“ Arquivo de log de exemplo (pode ser substituÃ­do por logs reais).
- `/output/` â€“ Pasta onde os resultados processados sÃ£o exportados como CSV.

---

## ğŸ“Œ Etapas da AnÃ¡lise

1. **Leitura dos Dados**
   - ImportaÃ§Ã£o do arquivo de log em CSV com Spark.

2. **Limpeza e PrÃ©-processamento**
   - RemoÃ§Ã£o de valores nulos ou invÃ¡lidos
   - ConversÃ£o de tipos e datas

3. **AnÃ¡lises Realizadas**
   - ğŸ“… Quantidade de acessos por dia
   - ğŸ“„ Top pÃ¡ginas mais acessadas
   - ğŸ“ Agrupamentos por IP, data e rota

4. **ExportaÃ§Ã£o de Resultados**
   - Salvando os resultados em formato `.csv` com particionamento se necessÃ¡rio

---

## ğŸ“Š Exemplos de Resultados

- **Top pÃ¡ginas acessadas:**
  ```plaintext
  /home
  /login
  /produtos
